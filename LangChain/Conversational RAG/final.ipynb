{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import bs4\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "#As the application getting complex, you should be able to inspect what exactly going on inside your chain or agents\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "#Loading\n",
    "loader = WebBaseLoader(\n",
    "    web_path=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\",\"post-header\",\"post-content\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "docs= loader.load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500,chunk_overlap=200).split_documents(docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "#Store Embeddings into Vector DB\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorDB= Chroma.from_documents(chunks,embeddings)\n",
    "#create a retriever for this DB\n",
    "retriever = vectorDB.as_retriever()\n",
    "# create a system prompt\n",
    "system_prompt= \" You are acting as an assistant to Question-Answering Tasks. Use the context given in triple backticks to answer the question.```{context}``` If you dont know answer jut say Dont know. Keep the answer as short using only 4 sentences  \"\n",
    "#prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_prompt),\n",
    "        (\"human\",\"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "stuff_chain = create_stuff_documents_chain(llm,prompt)\n",
    "# response = create_retrieval_chain(retriever,stuff_chain)\n",
    "# response[\"answer\"]\n",
    "#Upto here, It doesn't store your previous conversation what you had.\n",
    "#below we have implemented in such a way that it stores your previous chat and understands the context\n",
    "\n",
    "context_based_system_prompt =(\n",
    "        \"Latest question and Chat History has been given\"\n",
    "        \"This latest questin might reference the context in the chat history\"\n",
    "        \"Formuate a standalone question in a undestandable way with out the chat history\"\n",
    "        \"Do not Answer the question just reformulate it if needed, else, return as it is\"\n",
    "        )\n",
    "context_based_prompt = ChatPromptTemplate.from_messages(\n",
    "   [\n",
    "        (\"system\",context_based_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\",\"{input}\"),\n",
    "   ] \n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_retriever = create_history_aware_retriever(llm,retriever,context_based_prompt)\n",
    "chat_qa_prompt=ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\",system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\",\"{input}\"),\n",
    "         ]\n",
    "    \n",
    ")\n",
    "stuff_qa_chain = create_stuff_documents_chain(llm,chat_qa_prompt)\n",
    "\n",
    "final_rag_chain = create_retrieval_chain(history_retriever,stuff_qa_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "\n",
    "store={}\n",
    "#to get session\n",
    "def get_SessionHistory(session_id:str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stateful_rag_chain = RunnableWithMessageHistory(\n",
    "    final_rag_chain,\n",
    "    get_SessionHistory,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 314075b6-9fca-40ad-9bcf-0b1e392592a7 not found for run 995de541-5849-451e-bea3-f1b912cf3db7. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps models tackle hard tasks by transforming them into more manageable subtasks. It can be implemented through simple prompts for language models, task-specific instructions, or human inputs to guide the model's thinking process.\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =stateful_rag_chain.invoke(\n",
    "    {\"input\":\"What is Task Decompossion?\"},\n",
    "    config={\n",
    "        \"configurable\":{\"session_id\":\"xyz123\"}\n",
    "    },\n",
    "\n",
    ")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run de07e289-413c-405a-ac9c-7ac4b27178fb not found for run dca52ea0-8539-4f2a-8d8d-17036c19dbb2. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition can be achieved through simple prompting for language models, such as instructing them to think step by step or providing subgoals for a specific task. Task-specific instructions tailored to the nature of the task, like writing a story outline for novel writing, can also guide the decomposition process. Additionally, human inputs can be used to break down complex tasks into smaller steps for the model to follow.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response =stateful_rag_chain.invoke(\n",
    "    {\"input\":\"What are common ways of doing it?\"},\n",
    "    config={\n",
    "        \"configurable\":{\"session_id\":\"xyz123\"}\n",
    "    },\n",
    "\n",
    ")\n",
    "response[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: What is Task Decompossion?\n",
      "\n",
      "Assistant: Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps models tackle hard tasks by transforming them into more manageable subtasks. It can be implemented through simple prompts for language models, task-specific instructions, or human inputs to guide the model's thinking process.\n",
      "\n",
      "You: What are common ways of doing it?\n",
      "\n",
      "Assistant: Task decomposition can be achieved through simple prompting for language models, such as instructing them to think step by step or providing subgoals for a specific task. Task-specific instructions tailored to the nature of the task, like writing a story outline for novel writing, can also guide the decomposition process. Additionally, human inputs can be used to break down complex tasks into smaller steps for the model to follow.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for message in store[\"xyz123\"].messages:\n",
    "    if isinstance(message, AIMessage):\n",
    "        prefix = \"Assistant\"\n",
    "    else:\n",
    "        prefix = \"You\"\n",
    "\n",
    "    print(f\"{prefix}: {message.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
